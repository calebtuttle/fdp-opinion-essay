
## Fair Data Protocol and Data Markets

Fair Data Protocol (FDP) is taking principled steps toward a web in which users control their own data, but allowing users to sell access to their data through data markets introduces the risk of de-anonymization attacks. Removing this kind of data market removes the significant risk of users’ identities being discovered by malicious actors. However, data markets fulfill a purpose; they provide data to creators of machine learning (ML) models, and they can generate revenue for users. How can we retain the benefits of data markets without exposing user data? Instead of using a data market in which purchasers gain access to the unencrypted data, FDP might consider creating or integrating with a marketplace that utilizes privacy-preserving AI techniques. With privacy-preserving AI, users don’t need to expose their personal data to purchasers of data and purchasers can still benefit from access to large datasets.

What is the issue with a data market in which a user grants access to a data purchaser? De-anonymization attacks. In a de-anonymization attack, an attacker uses the patterns of different datasets to determine the identity of a user. For example, [a public voter database was used to de-anonymize patient records from a hospital in 1997](https://www.google.com/url?q=https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1748-720X.1997.tb01885.x&sa=D&source=docs&ust=1646448547060322&usg=AOvVaw3Zr-atCS3XBRIa394lR0RI). Notably, [algorithms have been proposed which can de-anonymize users even when the data is sparse and somewhat noisy](https://www.google.com/url?q=https://systems.cs.columbia.edu/private-systems-class/papers/Narayanan2008Robust.pdf&sa=D&source=docs&ust=1646448547061005&usg=AOvVaw2X2wcuo5RZk2KwBf-U0lGu). De-anonymization attacks are a serious risk when all kinds of user data might be available through a data market. Currently, there is user data on movie preferences, housing searches, dating preferences, health-related searches, and much, much more. There are few, if any, dApps powered by this amount of data, but if web3 dApps are going to provide the same services as the best web2 apps, it is likely that they will rely on similar quantities of data. In other words, even though there is currently little risk of de-anonymization attacks using user data from web3 dApps, the risk will likely increase as modern ML-powered dApps emerge. If someone can join two datasets from the 1990s to de-anonymize patient records, a sophisticated actor with data from modern apps could determine not only the identities of users but also other sensitive details, such as someone’s location, health conditions, and relationship to others. 

Fortunately, as concerns over privacy have increased, there have also been increases in privacy-preserving methodologies and cryptography, including techniques and technologies for privacy-preserving AI. On this front, there is [homomorphic encryption](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Homomorphic_encryption&sa=D&source=docs&ust=1646451998673188&usg=AOvVaw2U2xmk4iAan92vA_zjHZ4b), which allows encrypted data to be computed over and generates an encrypted result; [differential privacy](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Differential_privacy&sa=D&source=docs&ust=1646452155716128&usg=AOvVaw3olPKyXvdpf7CgcE09fmuW), which encompasses a broad set of methods that preserve privacy by only showing the result of running an algorithm on a dataset without allowing knowledge of individual items within the dataset; [secure multi-party computation](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Secure_multi-party_computation&sa=D&source=docs&ust=1646452484816185&usg=AOvVaw0mdfiBxm42wGwN8KCfl1d0), which is a set of methods that allow “a set of parties with private inputs wishes to jointly compute some function of their inputs” ([source](https://www.google.com/url?q=https://www.semanticscholar.org/paper/Secure-Multiparty-Computation-for-Data-Mining-Lindell-Pinkas/a6f644f6e739fa73ada11dc4c85b812b31f63d53&sa=D&source=docs&ust=1646467383486677&usg=AOvVaw2PX5NcQ6neKVdTUbYySFJZ)); and others. [Likely, a combination of these technologies should be used to preserve data privacy](https://www.google.com/url?q=https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5&sa=D&source=docs&ust=1646467383493635&usg=AOvVaw30Th1R05i9wOBtyPOBYgzv). A distributed, web3 data and computation marketplace whose architecture includes these mechanisms for privacy will conceivably service the needs of users better than a marketplace for raw data.

The architecture of a privacy-preserving data market will no doubt require the integration of varied technologies. Since the technologies and methods are relatively new, any proposals today will likely need to be incomplete and maybe even wrong. Nonetheless, I think it’s worth exploring what might compose the architecture of a privacy-preserving data market. A couple projects indicate what a decentralized privacy-preserving data market might look like. [Ocean Protocol](https://www.google.com/url?q=https://oceanprotocol.com&sa=D&source=docs&ust=1646531712096646&usg=AOvVaw1SwOHabkKLvnTpwd747QL_) has a compute-to-data protocol which is designed such that compute services occur on a “provider” node. A user can use Ocean to sell, to someone training an ML model, temporary compute-access to their data, and since computation occurs off-premise, the person with the ML model never sees the data. Another project, [PlatON](https://www.google.com/url?q=https://www.platon.network/en&sa=D&source=docs&ust=1646531787320099&usg=AOvVaw1h2G3XYSeu8qiGAx7MSppC), is integrating privacy-preserving ML libraries, such as those from [OpenMined](https://www.google.com/url?q=https://www.openmined.org&sa=D&source=docs&ust=1646532172134132&usg=AOvVaw2gqP_BKCl3OzLnGYpgHRg5), to tackle decentralized data markets where privacy is preserved. In a decentralized marketplace where compute and data are entangled, we will likely also need technologies such as [verifiable computing](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Verifiable_computing&sa=D&source=docs&ust=1646537750889125&usg=AOvVaw0RRnvoUJu4I5UNbULG8hER), which allows an actor to verify that another actor has run a certain computation. We might see data DAOs adopt secure multi-party computation where most users run their own nodes. We might see more open source ML models trained using [federated learning](https://www.google.com/url?q=https://en.wikipedia.org/wiki/Federated_learning&sa=D&source=docs&ust=1646537750889464&usg=AOvVaw3AQPQjhJ2LhtlXugHtbwHo) on user data; the designer of the ML model would never need to see the data because the dApp (and model training) could run in one user’s browser, then another’s browser, and so on before the trained model returns to the model designer. These are just some initial explorations (from an amateur) of how we could make use of user data and still preserve privacy in a decentralized way. 

In sum, it seems possible for FDP to retain the benefits of data markets without exposing user data. The risks of de-anonymization attacks should be considered when developing a data protocol for web3. This is not to say data markets should be eliminated. Rather, users should be prompted to consider very carefully whether they want to sell access to their raw data. A better default for monetizing one’s data might be a marketplace that utilizes privacy-preserving AI techniques. FDP might develop these libraries and products itself, or it might integrate with another project. This is a massive challenge (which might be addressed by the coming FairOS-compute), but it is worth pursuing as FDP aims for a truly self-sovereign and privacy-preserving internet.

### License
License: MIT
Copyright 2022 Caleb Tuttle
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

